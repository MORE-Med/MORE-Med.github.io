<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
		background-color : #EFEFEF;
	}
	.content{
    width : 1110px;
    padding : 25px 50px;
    margin : 25px auto;
    background-color : #fff;
    box-shadow: 0px 0px 10px #999;
    border-radius: 15px; 
	}
	h1 {
		font-weight:300;
	}

	.row {
		display: flex;
	  }
	  
	/* Create three equal columns that sits next to each other */
	.column {
	flex: 50%;
	padding: 5px;
	}

    .imageDiv img { 
        box-shadow: 1px 1px 10px rgb(255, 255, 255); 
        margin: 2px;
        max-height: 50px;
        cursor: pointer;
    display:inline-block;
    *display:inline;
    *zoom:1;
    vertical-align:top;
    }


	tr.spaceUnder>td {
		padding-bottom: 3em;
	}

	.paddingBetweenRows td
	{
		padding:0 15px;
		border-spacing: 100px;
	}





    /*
      Rollover Image
     */
	 .figure {
        position: relative;
        max-width: 100%;
    }
    .figure img.image-hover {
      position: absolute;
      top: 0;
      right: 0;
      left: 0;
      bottom: 0;
      object-fit: contain;
      opacity: 0;
      transition: opacity .2s;
    }
    .figure:hover img.image-hover {
      opacity: 1;
    }
	
	* {box-sizing:border-box}

	/* Slideshow container */
	.slideshow-container {
	max-width: 232;
	position: relative;
	margin: auto;
	}

	/* Hide the images by default */
	.mySlides {
	display: none;
	}

	/* Next & previous buttons */
	.prev, .next {
	cursor: pointer;
	position: absolute;
	top: 50%;
	width: auto;
	margin-top: -22px;
	padding: 16px;
	color: white;
	font-weight: bold;
	font-size: 18px;
	transition: 0.6s ease;
	border-radius: 0 3px 3px 0;
	user-select: none;
	}

	/* Position the "next button" to the right */
	.next {
	right: 0;
	border-radius: 3px 0 0 3px;
	}

	/* On hover, add a black background color with a little bit see-through */
	.prev:hover, .next:hover {
	background-color: rgba(0,0,0,0.8);
	}

	/* Caption text */
	.text {
	color: #f2f2f2;
	font-size: 15px;
	padding: 8px 12px;
	position: absolute;
	bottom: 8px;
	width: 100%;
	text-align: center;
	}

	/* Number text (1/3 etc) */
	.numbertext {
	color: #f2f2f2;
	font-size: 12px;
	padding: 8px 12px;
	position: absolute;
	top: 0;
	}

	/* The dots/bullets/indicators */
	.dot {
	cursor: pointer;
	height: 15px;
	width: 15px;
	margin: 0 2px;
	background-color: #bbb;
	border-radius: 50%;
	display: inline-block;
	transition: background-color 0.6s ease;
	}

	.active, .dot:hover {
	background-color: #717171;
	}

	/* Fading animation */
	.fade {
	}

	@keyframes fade {
	from {opacity: .4}
	to {opacity: 1}
	}




	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	table {
		border-collapse: collapse;
	}
	.vtable {
		border-collapse:separate;
		border-spacing:10px 15px;
	}
	.rtable {
		margin-left: 2em;
		border-collapse:separate;
		border-spacing:10px 15px;
	}
</style>

<html>
  <head>
		<title>Touch and Go</title>
		<meta property="og:title" content="Touch and Go" />
  </head>

  <body>
	<div class="content">
		<br>
          <center>
          	<span style="font-size:36px">Touch and Go: Learning from Human-Collected Vision and Touch</span>
			<br><br>
	  		  <table align=center width=700px>
	  			  <tr>
	  	              <td align=center width=200px>
	  					<center><span style="font-size:20px"><a href="https://fredfyyang.github.io/">Fengyu Yang*</a></span></center>
		  		  	  </td>
	  	              <td align=center width=200px>
	  					<center><span style="font-size:20px"><a href="https://www.linkedin.com/in/chenyang-ma-66945091/">Chenyang Ma*</a></span>	</center>
		  		  	  </td>
	  	              <td align=center width=200px>
	  					<center><span style="font-size:20px"><a href="https://www.linkedin.com/in/jiacheng-zhang-689b8319a/?trk=public_profile_browsemap">Jiacheng Zhang</a></span></center>
		  		  	  </td>
					  
					  <td align=center width=200px>
						<center><span style="font-size:20px"><a href="https://jwzhi.github.io/">Jing Zhu</a></span></center>
					  </td>

			  </table>
			  <table align=center width=400px>
				  <tr>
					<td align=center width=200px>
					  <center><span style="font-size:20px"><a href="http://robotouch.ri.cmu.edu/yuanwz/">Wenzhen Yuan</a></span></center>
					</td>

					<td align=center width=200px>
					  <center><span style="font-size:20px"><a href="https://andrewowens.com/">Andrew Owens</a></span></center>
					</td>
			</table>
			<br>
          		<span style="font-size:20px"> NeurIPS 2022 Datasets and Benchmarks</span>
			<br><br>
	  		  <table align=center width=650px>
	  			  <tr>
	  	              <td align=center width=150px>
	  					<center>
							<span style="font-size:20px"><a href='https://arxiv.org/abs/2211.12498'> [Paper]</a></span>
							&nbsp&nbsp&nbsp&nbsp&nbsp
	  						<span style="font-size:20px"><a href='https://drive.google.com/drive/folders/1NDasyshDCL9aaQzxjn_-Q5MBURRT360B'> [Dataset]</a></span>
							  &nbsp&nbsp&nbsp&nbsp&nbsp
	  						<span style="font-size:20px"><a href='https://github.com/fredfyyang/Tactile-Driven-Image-Stylization/'> [Code]</a></span>
		  		  		</center>
		  		  	  </td>
	  			  <tr>
			  </table>
          </center>
		  <p><strong>Quick Jump:</strong></p>
			<ol>
				<li><a href="#overview">Overview</a></li>
				<li><a href="#demonstration video">Examples from our Dataset</a></li>
				<li><a href="#dataset">Dataset</a></li>
				<li><a href="#applications">Applications</a></li>
				<li><a href="#comparison">Comparison to Other Datasets</a></li>
			</ol>
	</div>
    

	<div class="content" id="overview">
		<table align=center width=850px>
			<center><h1>Overview</h1></center>
		</table>

		We propose a dataset for multimodal visuo-tactile learning called Touch and Go, in which human data collectors probe objects in natural environments with tactile sensors, while recording egocentric video. 
		<br>We successfully apply it to a variety of tasks: 1) self-supervised visuo-tactile feature learning, 2) the novel task of tactile-driven image stylization, i.e., making an object look as though it were ''felt like'' a given tactile input, and 3) predicting future frames of a tactile signal from visuo-tactile inputs.
		
		<center><h3>Human-Powered Data Collection</h3></center>
		<div class="row">

			<div class="column" style="padding-left:50px">
				<!-- <video controls class="html-video" width="512" height="201.6"><source src="./resources/video/comb_long1.mp4" type="video/mp4"></video>
				<br><br>
				<video controls class="html-video" width="512" height="201.6"><source src="./resources/video/comb_long2.mp4" type="video/mp4"></video> -->
				<p style="font-size:18px">
					&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
					 Vision &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
					&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Touch
				</p>
				<iframe width="512" height="403.2" src="https://www.youtube.com/embed/OjTD3Z27gcM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
				<br>
				<span style="font-size:16px"><i>Untrimmed video examples from our dataset</i>	
			</div>
			&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
			<div class="column">	
			<br>
			<br>
				<img id="human_collect" src='./resources/images/human_collect2.jpg' width="300" height="400"/>
				<br>
				<span style="font-size:16px"><i>Two people collecting data</i>
			</div>
		</div>

		
	</div>

		  

	<div class="content" id="demonstration video">
		<center><h1>Examples from our Dataset</h3></center>
		<center><iframe width="920" height="483" src="https://www.youtube.com/embed/6kdPmmWrGz4"> </iframe><center>
	</div>

	<div class="content" id="dataset">
	<table align=center width=850px>
		<center><h1>Dataset</h1></center>
	</table>
	<center style="font-size:20px"><a href='https://drive.google.com/drive/folders/1NDasyshDCL9aaQzxjn_-Q5MBURRT360B'> [Download Link]</a></center><br>
	<center><h3>Labeled Video Data</h3></center>
	<table width=45% class="vtable" style="float: left">
		<tr>
			<th style="font-size:16px"> <p>Label</p></th>
			<th style="font-size:16px" >RGB Video &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Gelsight Video</th>
		</tr>

		<tr>
			<th style="font-size:16px">Synthetic Fabric</th>
			<th><video controls class="html-video" width="384" height="151.2"><source src="./resources/video/fabric.mp4" type="video/mp4"></video></th>
		</tr>
		
		<tr>
			<th style="font-size:16px">Stone</th>
			<th><video controls class="html-video" width="384" height="151.2"><source src="./resources/video/stone_1.mp4" type="video/mp4"></video></th>
		</tr>

		<tr>
			<th style="font-size:16px">Brick</th>
			<th><video controls class="html-video" width="384" height="151.2"><source src="./resources/video/brick.mp4" type="video/mp4"></video></th>
		</tr>
		</table>


		<table width=45% class="rtable" style="float: right">
			<tr>
				<th style="font-size:16px"> <p>Label</p></th>
				<th style="font-size:16px" >RGB Video &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Gelsight Video</th>
			</tr>
	
			<tr>
				<th style="font-size:16px">Wood</th>
				<th><video controls class="html-video" width="384" height="151.2"><source src="./resources/video/wood.mp4" type="video/mp4"></video></th>
			</tr>
			
			<tr>
				<th style="font-size:16px">Grass</th>
				<th><video controls class="html-video" width="384" height="151.2"><source src="./resources/video/grass.mp4" type="video/mp4"></video></th>
			</tr>
	
			<tr>
				<th style="font-size:16px">Concrete</th>
				<th>
					<video controls class="html-video" width="384" height="151.2"><source src="./resources/video/concrete_1.mp4" type="video/mp4"></video>
				</th>
			</tr>
	
			</table>
		

        <div align=center width=100%>
            <p  style="text-align: left; width: 100%" >
                <b>Touch and Go Dataset</b> &nbsp &nbsp &nbsp We collect a dataset of natural vision-and-touch signals. Our dataset contains multimodal data
				recorded by humans, who probe objects in their natural locations with a tactile sensor. To more easily
				train and analyze models on this dataset, we also collect material labels and identify touch onsets.<br>
				The <tt>touch_and_go directory</tt> contains a directory of raw videos, <tt>extract_frame.py</tt> that convert raw videos to frames, and <tt>label.txt</tt> of material labels for onset frames.
				<br><br>
				Each raw video folder in the <em>Dataset</em> folder consists of six items:<br><br>
				&emsp;&emsp;<item><tt>video.mp4</tt>:  Raw RGB video recording the interaction of human probing objects.</item><br>
				&emsp;&emsp;<item><tt>gelsight.mp4</tt>: Raw GelSight (tactile) video for objects.</item><br>
				&emsp;&emsp;<item><tt>time1.npy</tt>:  The recording time for each frame in ''video.mp4''.</item><br>
				&emsp;&emsp;<item><tt>time2.npy</tt>:  The recording time for each frame in ''gelsight.mp4''.</item><br>
				&emsp;&emsp;<item><tt>video_frame</tt>:  The folder containing all the frames in ''video.mp4''. (Generated after running <tt>extract_frame.py</tt>)</item><br>
				&emsp;&emsp;<item><tt>gelsight_frame</tt>:  The folder containing all the frames in ''gelsight.mp4''. (Generated after running <tt>extract_frame.py</tt>)</item><br>
            </p>
        </div>
	</div>




	
	<div class="content" id="applications">
  		  <center><h1>Applications</h1></center>
			To evaluate the effectiveness of our dataset, we perform tasks that are designed to span a variety of application domains, including representation learning, image synthesis, and future prediction.

			<center><h3>Tactile-driven image stylization</h3></center> 
			Both touch and sight convey material properties and geometry. A model that can successfully predict these properties from visuo-tactile data therefore ought to be able to translate between modalities.
			We propose the task of tactile-driven image stylization: making an image look as though it “feels like” a given touch signal. In the figure below we show results from our model. Our model successfully manipulates
			images to match tactile inputs, such as by making surfaces rougher or smoother, or by creating
			“hybrid” materials (e.g., adding grass to a surface).
			<br><br>
			<b>Hover over each row to make the right pictures look like the left tactile input.</b>
			<br><br>

			<table align=center style="width:100%" class='paddingBetweenRows'>
			  <tr class="spaceUnder">
				  <td style="padding-left:6em">
					<p style="font-size:20px">&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Conditional tactile example</p>
					<a href="./resources/images/stone_eg.png"><img name="stone" src = "./resources/images/stone_eg.png" height="160px"></img></href></a></td>
				  <td>
						  <p style="font-size:20px" id="stone text">&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Input Image</p>
						  <img name="stone" id="stone_img2" src='./resources/images/to_stone/2.png'  height="160px"/>
						  <img name="stone" id="stone_img3" src='./resources/images/to_stone/3.png'  height="160px"/>
						  <!-- <img id="stone_img5" src='./resources/images/to_stone/5.png'  height="100px"/> -->
				  </td>
			  </tr>
  
			  <tr class="spaceUnder">
				  <td style="padding-left:6em">
					  <br>
					  <a href="./resources/images/concrete_eg.png"><img name="concrete" id="concrete" src = "./resources/images/concrete_eg.png" height="160px"></img></href></a>
					  <td>
						  <p style="font-size:20px" id="concrete text">&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Input Image</p>
						  <img name="concrete" id="concrete_img3" src='./resources/images/to_concrete/3.png' height=160px/>
						  <img name="concrete" id="concrete_img4" src='./resources/images/to_concrete/4.png' height=160px/>
						  <!-- <img id="concrete_img2" src='./resources/images/to_concrete/2.png' height=100px /> -->
				  </td>
			  </tr>
  
			  <tr class="spaceUnder">
				  <td style="padding-left:6em">
					<br>
					  <a href="./resources/images/fabric_eg.png"><img name="fabric" id="fabric" src = "./resources/images/fabric_eg.png" height="160px"></img></href></a>
					  <td>
						<p style="font-size:20px" id="fabric text">&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Input Image</p>
						  <img name="fabric" id="fabric_img2" src='./resources/images/to_fabric/2.png' height=160px />
						  <img name="fabric" id="fabric_img4" src='./resources/images/to_fabric/4.png' height=160px/>
						  <!-- <img id="fabric_img5" src='./resources/images/to_fabric/5.png' height=100px/> -->
				  </td>
			  </tr>
  
			  <tr class="spaceUnder">
				  <td style="padding-left:6em">
					<br>
					  <a href="./resources/images/grass_eg.png"><img name="grass" id="grass" src = "./resources/images/grass_eg.png" height="160px"></img></href></a>
					  <td>
						<p style="font-size:20px" id="grass text">&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Input Image</p>
						  <img name="grass" id="grass_img3" src='./resources/images/to_grass/3.png' height=160px/>
						  <img name="grass" id="grass_img4" src='./resources/images/to_grass/4.png' height=160px/>
						  <!-- <img id="grass_img5" src='./resources/images/to_grass/5.png' height=100px/> -->
				  </td>
			  </tr>
  
			</table>
  
  
		  <script>
			  let stone = document.getElementsByName("stone");
			  stone.forEach(function(elem) {
				elem.addEventListener("mouseover", function() {
					document.getElementById("stone text").innerHTML='<p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Stylized Output</p>'
					document.getElementById("stone_img2").src='./resources/images/to_stone/2s.png';
					document.getElementById("stone_img3").src='./resources/images/to_stone/3s.png';
				});
			});
			  
			  stone.forEach(item => {
				item.addEventListener('mouseleave', event => {
					document.getElementById("stone text").innerHTML='<p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Input Image</p>'
					document.getElementById("stone_img2").src='./resources/images/to_stone/2.png' ;
					document.getElementById("stone_img3").src='./resources/images/to_stone/3.png' ;
				})
			  })

		  </script>


  
		  <script>
			  const concrete = document.getElementsByName("concrete");
			  
			  concrete.forEach(function(elem) {
				elem.addEventListener("mouseover", function() {
					document.getElementById("concrete text").innerHTML='<p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Stylized Output</p>'
					document.getElementById("concrete_img3").src='./resources/images/to_concrete/3s.png';
					document.getElementById("concrete_img4").src='./resources/images/to_concrete/4s.png';
				});
			});
			  

			concrete.forEach(function(elem) {
				elem.addEventListener("mouseleave", function() {
					document.getElementById("concrete text").innerHTML='<p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Input Image</p>'
					document.getElementById("concrete_img3").src='./resources/images/to_concrete/3.png';
					document.getElementById("concrete_img4").src='./resources/images/to_concrete/4.png';
				});
			});
		  </script>
  
		  <script>
			const fabric = document.getElementsByName("fabric");
			  
			fabric.forEach(function(elem) {
				elem.addEventListener("mouseover", function() {
					document.getElementById("fabric text").innerHTML='<p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Stylized Output</p>'
					document.getElementById("fabric_img2").src='./resources/images/to_fabric/2s.png';
					document.getElementById("fabric_img4").src='./resources/images/to_fabric/4s.png';
				});
			});

			fabric.forEach(function(elem) {
				elem.addEventListener("mouseleave", function() {
					document.getElementById("fabric text").innerHTML='<p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Input Image</p>'
					document.getElementById("fabric_img2").src='./resources/images/to_fabric/2.png' ;
					document.getElementById("fabric_img4").src='./resources/images/to_fabric/4.png' ;
				});
			});
		  </script>
  
		  <script>
			  const grass = document.getElementsByName("grass");
			  grass.forEach(function(elem) {
				elem.addEventListener("mouseover", function() {
					document.getElementById("grass text").innerHTML='<p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Stylized Output</p>'
					document.getElementById("grass_img3").src='./resources/images/to_grass/3s.png';
					document.getElementById("grass_img4").src='./resources/images/to_grass/4s.png';
				});
			});

  
			  grass.forEach(function(elem) {
				elem.addEventListener("mouseleave", function() {
					document.getElementById("grass text").innerHTML='<p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Input Image</p>'
					document.getElementById("grass_img3").src='./resources/images/to_grass/3.png';
					document.getElementById("grass_img4").src='./resources/images/to_grass/4.png';
				});
			});
		  </script>
  

			<center><h3>Multimodal video prediction</h3></center>
			We use our dataset to ask whether visual data can improve our estimates of future tactile signals: i.e., what will this object
			feel like in a moment?<br>
			We predict multiple frames by autoregressively feeding our output images back to the original model.
			We evaluate our model for predicting future tactile signals. In the figure below, we compare a tactile-only model to a multimodal visuo-tactile model, and show that the latter obtains better performance.
			By incorporating our dataset's visual signal, the model gains a constant performance increase
			under different evaluation metrics, under both experimental settings. The gap becomes larger for
			longer time horizon, suggesting that visual information may be more helpful in this case.
			<br><br>
			<center>
				<a href="./resources/images/video_pred1.png"><img class="rounded" src = "./resources/images/video_pred1.png" height="360px" width="900px"></img></href></a><br>
			</center>
		
	</div>
		
	<div class="content" id="comparison">
			<center><h1>Comparison to Other Datasets</h1></center>
			To help understand the differences between our dataset and those of previous work: Object Folder 2.0, which contains virtual objects, and two robotic datasets: 
			Feeling of Success, and VisGel.
			We show examples from indoor scenes, since the other datasets do not contain outdoor scenes, and with rigid materials (since the virtual scenes do not contain deformable materials). Each row illustrates objects which are composed of similar materials, along with their corresponding GelSight images.
		
		
		<center>
			<a href="./resources/images/comparison_new.png"><img class="rounded" src = "./resources/images/comparison_new.png" object-fit='cover' height="440px" width="1000px"></img></href></a><br>
		</center>
		<td width=400px>
			<center>
				<span style="font-size:14px"><i>We provide qualitative examples of 
					visual and tactile data from other datasets (left), along with examples 
					from similar material taken from our dataset (right).</i>
		  </center>
		</td>
	</div>
		
			
	<div class="content" id="acknowledgements">
		<center><h1>Acknowledgements</h1></center>
		We thank Xiaofeng Guo and Yufan Zhang for the extensive help with the GelSight sensor, and thank Daniel Geng, Yuexi Du and Zhaoying Pan for the helpful discussions. This work was supported in part by Cisco Systems.
		The webpage template was adopted from <a href="http://richzhang.github.io/colorization/">Colorization</a> project and <a href="http://www.cs.columbia.edu/~vondrick/ihog/">HOGgles</a> project.
	</div>
		<hr>
		<br><br><br>
		<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" /></a><br /><span xmlns:dct="http://purl.org/dc/terms" href="http://purl.org/dc/dcmitype/MovingImage" property="dct:title" rel="dct:type">Touch and Go: Learning from Human-Collected Vision and Touch</span> by <span xmlns:cc=“http://creativecommons.org/ns#” property=“cc:attributionName”>Fengyu Yang, Chenyang Ma, Jiacheng Zhang, Jing Zhu, Wenzhen Yuan, Andrew Owens</span> is licensed under a <a rel=“license” href=“http://creativecommons.org/licenses/by/4.0/”>Creative Commons Attribution 4.0 International License</a>
	
              
</body>
</html>
 
